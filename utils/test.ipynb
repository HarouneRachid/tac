{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeeb202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install streamlit bertopic scikit-learn matplotlib pandas sentence-transformers nltk seaborn WordCloud\n",
    "# pip install -U kaleido\n",
    "# pip install transformers torch -> pour utiliser Camembert\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.cluster import KMeans\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from io import StringIO, BytesIO\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from umap import UMAP  # Import UMAP for dimensionality reduction\n",
    "import plotly.express as px  # Import plotly for bubble chart visualization\n",
    "import numpy as np\n",
    "import os  # Pour gérer les opérations sur le système de fichiers\n",
    "import datetime\n",
    "\n",
    "# Télécharger le corpus de stop words si nécessaire\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Utiliser les stop words français de NLTK\n",
    "french_stopwords = stopwords.words('french')\n",
    "\n",
    "\n",
    "# Fonction pour extraire le contenu des articles\n",
    "def parse_article(article_text):\n",
    "    lines = article_text.strip().split('\\n')\n",
    "    content = '\\n'.join(lines[1:]) if len(lines) > 1 else ''\n",
    "    return {'content': content}\n",
    "\n",
    "\n",
    "# Fonction pour prétraiter le texte\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Fonction pour créer le concordancier\n",
    "def create_concordance(df, clusters):\n",
    "    concordance_df = pd.DataFrame({\n",
    "        'Document': df['content'],\n",
    "        'Cluster': [f'Cluster {c + 1}' for c in clusters]\n",
    "    })\n",
    "    grouped_concordance = concordance_df.groupby('Cluster')['Document'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "    return grouped_concordance\n",
    "\n",
    "\n",
    "# Fonction pour télécharger un DataFrame en CSV\n",
    "def save_csv(dataframe, filename, directory):\n",
    "    \"\"\"Enregistre un DataFrame en CSV dans un répertoire donné.\"\"\"\n",
    "    path = os.path.join(directory, f\"{filename}.csv\")\n",
    "    dataframe.to_csv(path, index=False, encoding='utf-8')\n",
    "    st.success(f\"Enregistré : {path}\")\n",
    "\n",
    "\n",
    "# Fonction pour afficher et télécharger la matrice de similarité cosinus entre les clusters\n",
    "def display_similarity_matrix(embeddings, cluster_labels, directory):\n",
    "    # Calcul de la similarité entre les clusters\n",
    "    cluster_centers = [embeddings[cluster_labels == i].mean(axis=0) for i in range(max(cluster_labels) + 1)]\n",
    "    similarity_matrix = cosine_similarity(cluster_centers)\n",
    "\n",
    "    cluster_names = [f'Cluster {i + 1}' for i in range(len(cluster_centers))]\n",
    "\n",
    "    similarity_df = pd.DataFrame(similarity_matrix, columns=cluster_names, index=cluster_names)\n",
    "\n",
    "    st.write(\"Matrice de Similarité Cosinus entre les Clusters\")\n",
    "    st.dataframe(similarity_df)\n",
    "\n",
    "    save_csv(similarity_df, \"kmeans_cluster_similarity_matrix\", directory)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(similarity_df, cmap='coolwarm', ax=ax, annot=True, fmt=\".2f\", xticklabels=cluster_names,\n",
    "                yticklabels=cluster_names)\n",
    "    plt.title(\"Carte Thermique de Similarité entre les Clusters\")\n",
    "    plt.savefig(os.path.join(directory, \"similarity_heatmap.png\"))\n",
    "    st.pyplot(fig)\n",
    "    st.success(f\"Graphique enregistré : {os.path.join(directory, 'similarity_heatmap.png')}\")\n",
    "\n",
    "\n",
    "# Fonction pour afficher les nuages de mots pour chaque cluster\n",
    "def display_wordclouds(df, cluster_labels, directory):\n",
    "    for cluster in set(cluster_labels):\n",
    "        st.subheader(f\"Nuage de Mots pour le Topic {cluster + 1}\")\n",
    "        cluster_data = df['content'][cluster_labels == cluster]\n",
    "        wordcloud_text = ' '.join(cluster_data)\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white',\n",
    "                              stopwords=set(french_stopwords)).generate(wordcloud_text)\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Topic {cluster + 1}')  # Ajouter \"Topic n°\" au titre\n",
    "        plt.savefig(os.path.join(directory, f\"wordcloud_topic_{cluster + 1}.png\"))\n",
    "        st.pyplot(plt)\n",
    "        st.success(f\"Nuage de mots enregistré : {os.path.join(directory, f'wordcloud_topic_{cluster + 1}.png')}\")\n",
    "\n",
    "\n",
    "# Fonction pour obtenir le texte d'une URL\n",
    "def get_text_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            st.error(\"Impossible de récupérer le texte depuis l'URL.\")\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        st.error(f\"Erreur lors de la récupération du texte depuis l'URL : {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Fonction pour afficher la visualisation des clusters en 2D\n",
    "def display_cluster_visualization(embeddings, labels, directory):\n",
    "    # Réduire les dimensions des embeddings à 2D pour la visualisation\n",
    "    umap_model = UMAP(n_components=2, random_state=42)\n",
    "    reduced_embeddings = umap_model.fit_transform(embeddings)\n",
    "\n",
    "    # Créer un DataFrame pour la visualisation\n",
    "    viz_df = pd.DataFrame({\n",
    "        'x': reduced_embeddings[:, 0],\n",
    "        'y': reduced_embeddings[:, 1],\n",
    "        'Cluster': labels\n",
    "    })\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.scatterplot(data=viz_df, x='x', y='y', hue='Cluster', palette='viridis', s=50, alpha=0.7)\n",
    "    plt.title(\"Visualisation des Clusters K-Means\")\n",
    "    plt.xlabel(\"Dimension 1\")\n",
    "    plt.ylabel(\"Dimension 2\")\n",
    "    plt.legend(title='Clusters', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.savefig(os.path.join(directory, \"kmeans_cluster_2D.png\"))\n",
    "    st.pyplot(plt)\n",
    "    st.success(f\"Graphique 2D enregistré : {os.path.join(directory, 'kmeans_cluster_2D.png')}\")\n",
    "\n",
    "\n",
    "# Fonction pour visualiser les centroides des clusters\n",
    "def display_centroid_visualization(embeddings, cluster_labels, directory):\n",
    "    # Calculer les centroids des clusters\n",
    "    cluster_centers = np.array([embeddings[cluster_labels == i].mean(axis=0) for i in range(max(cluster_labels) + 1)])\n",
    "\n",
    "    # Réduire les dimensions des centroids à 2D pour la visualisation\n",
    "    umap_model = UMAP(n_components=2, random_state=42)\n",
    "    reduced_centroids = umap_model.fit_transform(cluster_centers)\n",
    "\n",
    "    # Créer un DataFrame pour la visualisation\n",
    "    df_centroids = pd.DataFrame({\n",
    "        'x': reduced_centroids[:, 0],\n",
    "        'y': reduced_centroids[:, 1],\n",
    "        'Cluster': range(1, len(cluster_centers) + 1),\n",
    "        'Size': [10] * len(cluster_centers)  # La taille des bulles peut être personnalisée\n",
    "    })\n",
    "\n",
    "    fig = px.scatter(\n",
    "        df_centroids, x='x', y='y', size='Size', color='Cluster',\n",
    "        title='Visualisation des Centroides des Clusters',\n",
    "        labels={'x': 'Dimension 1', 'y': 'Dimension 2', 'Cluster': 'Clusters'},\n",
    "        hover_data={'Size': False}\n",
    "    )\n",
    "    fig.update_traces(marker=dict(opacity=0.6))\n",
    "    st.plotly_chart(fig)\n",
    "    fig.write_image(os.path.join(directory, \"kmeans_centroid_visualization.png\"))\n",
    "    st.success(f\"Graphique des centroids enregistré : {os.path.join(directory, 'kmeans_centroid_visualization.png')}\")\n",
    "\n",
    "\n",
    "# Fonction pour visualiser les clusters regroupés en bulles\n",
    "def display_grouped_bubble_chart(embeddings, cluster_labels, directory):\n",
    "    umap_model = UMAP(n_components=2, random_state=42)\n",
    "    reduced_embeddings = umap_model.fit_transform(embeddings)\n",
    "    df = pd.DataFrame({\n",
    "        'x': reduced_embeddings[:, 0],\n",
    "        'y': reduced_embeddings[:, 1],\n",
    "        'Cluster': cluster_labels\n",
    "    })\n",
    "\n",
    "    # Calculer la taille de chaque bulle en fonction du nombre de points dans chaque cluster\n",
    "    cluster_sizes = df['Cluster'].value_counts().sort_index()\n",
    "    df['Size'] = df['Cluster'].map(cluster_sizes)\n",
    "\n",
    "    fig = px.scatter(\n",
    "        df, x='x', y='y', size='Size', color='Cluster',\n",
    "        hover_data=['Cluster'], opacity=0.6, size_max=50,\n",
    "        title='Visualisation des Clusters Regroupés en Forme de Bulles'\n",
    "    )\n",
    "    fig.update_layout(showlegend=True)\n",
    "    st.plotly_chart(fig)\n",
    "    fig.write_image(os.path.join(directory, \"kmeans_grouped_bubble_chart.png\"))\n",
    "    st.success(\n",
    "        f\"Graphique des bulles regroupées enregistré : {os.path.join(directory, 'kmeans_grouped_bubble_chart.png')}\")\n",
    "\n",
    "\n",
    "# Fonction principale pour l'analyse\n",
    "def main():\n",
    "    # Titre de l'application\n",
    "    st.set_page_config(page_title=\"Analyse textuelle avec K-means\")\n",
    "    st.title(\"Analyse textuelle avec K-means\")\n",
    "    st.markdown(\n",
    "        \"**Version du script : 1.0 - Date : 09-08-2024 - Stéphane Meurisse - [www.codeandcortex.fr](https://www.codeandcortex.fr)**\")\n",
    "\n",
    "    # Menu principal\n",
    "    menu_principal = st.sidebar.radio(\n",
    "        \"Menu Principal\",\n",
    "        [\"Préparation des Données\", \"Analyse des Données\", \"FAQ\"]\n",
    "    )\n",
    "\n",
    "    # Initialiser le conteneur de session pour stocker le DataFrame et le nom du fichier\n",
    "    if 'df' not in st.session_state:\n",
    "        st.session_state.df = None\n",
    "    if 'file_name' not in st.session_state:\n",
    "        st.session_state.file_name = None\n",
    "    if 'save_directory' not in st.session_state:\n",
    "        st.session_state.save_directory = None\n",
    "\n",
    "    if menu_principal == \"Préparation des Données\":\n",
    "        st.sidebar.markdown(\"### Préparation des Données\")\n",
    "\n",
    "        # Sous-menu pour les options de préparation des données\n",
    "        preparation_option = st.sidebar.radio(\n",
    "            \"Options de Préparation\",\n",
    "            [\"Uploader un Fichier\", \"URL\"]\n",
    "        )\n",
    "\n",
    "        if preparation_option == \"Uploader un Fichier\":\n",
    "            st.subheader(\"Uploader un Fichier\")\n",
    "            uploaded_file = st.file_uploader(\"Téléchargez un fichier texte contenant des articles de presse\",\n",
    "                                             type=\"txt\")\n",
    "            if uploaded_file is not None:\n",
    "                # Stocker le nom du fichier\n",
    "                st.session_state.file_name = uploaded_file.name\n",
    "                stringio = StringIO(uploaded_file.getvalue().decode(\"utf-8\"))\n",
    "                raw_articles = stringio.read().strip().split('****')\n",
    "                articles_data = [parse_article(article) for article in raw_articles if article.strip()]\n",
    "                st.session_state.df = pd.DataFrame(articles_data)\n",
    "                st.session_state.df['content'] = st.session_state.df['content'].apply(preprocess_text)\n",
    "                st.write(st.session_state.df)\n",
    "                st.sidebar.text(f\"Fichier chargé : {st.session_state.file_name}\")\n",
    "\n",
    "        elif preparation_option == \"URL\":\n",
    "            st.subheader(\"URL\")\n",
    "            url = st.text_input(\"Entrez l'URL du texte\")\n",
    "            if url:\n",
    "                file_text = get_text_from_url(url)\n",
    "                raw_articles = file_text.strip().split('****')\n",
    "                articles_data = [parse_article(article) for article in raw_articles if article.strip()]\n",
    "                st.session_state.df = pd.DataFrame(articles_data)\n",
    "                st.session_state.df['content'] = st.session_state.df['content'].apply(preprocess_text)\n",
    "                st.write(st.session_state.df)\n",
    "                st.sidebar.text(f\"Fichier chargé depuis l'URL\")\n",
    "\n",
    "        if st.session_state.df is not None:\n",
    "            # Afficher le nombre total d'articles trouvés\n",
    "            total_articles = len(st.session_state.df)\n",
    "            st.markdown(f\"**Nombre d'articles trouvés : {total_articles}**\")\n",
    "\n",
    "            # Demander à l'utilisateur de définir un répertoire de sauvegarde\n",
    "            st.subheader(\"Définir le Répertoire de Sauvegarde\")\n",
    "            save_directory = st.text_input(\"Entrez le chemin du répertoire de sauvegarde\",\n",
    "                                           value=os.path.expanduser(\"~/Documents/AnalyseTextuelle\"))\n",
    "            if st.button(\"Définir le Répertoire\"):\n",
    "                if not os.path.exists(save_directory):\n",
    "                    os.makedirs(save_directory)\n",
    "                st.session_state.save_directory = save_directory\n",
    "                st.success(f\"Répertoire de sauvegarde défini : {save_directory}\")\n",
    "\n",
    "    elif menu_principal == \"Analyse des Données\":\n",
    "        st.sidebar.markdown(\"### Analyse des Données\")\n",
    "\n",
    "        # Vérification si les données sont chargées\n",
    "        if st.session_state.df is None:\n",
    "            st.error(\"Veuillez d'abord préparer vos données dans la section précédente.\")\n",
    "            return\n",
    "\n",
    "        # Vérification du répertoire de sauvegarde\n",
    "        if st.session_state.save_directory is None:\n",
    "            st.error(\"Veuillez d'abord définir un répertoire de sauvegarde dans la section de préparation des données.\")\n",
    "            return\n",
    "\n",
    "        # Afficher le nom du fichier chargé dans la barre latérale\n",
    "        if st.session_state.file_name:\n",
    "            st.sidebar.text(f\"Fichier chargé : {st.session_state.file_name}\")\n",
    "\n",
    "        # Utilisation des données préparées\n",
    "        df = st.session_state.df\n",
    "        save_directory = st.session_state.save_directory\n",
    "\n",
    "        st.subheader(\"KMeans\")\n",
    "        st.write(\"**KMeans**: Algorithme de clustering qui partitionne les données en un nombre fixe de clusters.\")\n",
    "\n",
    "        # Initialiser SentenceTransformer pour créer des embeddings\n",
    "        sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "        # Générer les embeddings pour chaque article\n",
    "        embeddings = sentence_model.encode(df['content'].tolist(), show_progress_bar=True)\n",
    "\n",
    "        # Paramètres min_df et max_df ajustables par l'utilisateur\n",
    "        st.sidebar.subheader(\"Paramètres du Vectorizer\")\n",
    "        min_df = st.sidebar.slider(\"Min DF (fraction minimale de documents)\", 0.0, 1.0, 0.01, 0.01)\n",
    "        max_df = st.sidebar.slider(\"Max DF (fraction maximale de documents)\", min_df, 1.0, 0.95, 0.01)\n",
    "\n",
    "        vectorizer_model = CountVectorizer(stop_words=french_stopwords, min_df=min_df, max_df=max_df,\n",
    "                                           ngram_range=(1, 3))\n",
    "\n",
    "        # Appliquer la méthode du coude pour déterminer le nombre optimal de clusters\n",
    "        st.subheader(\"Détermination du Nombre Optimal de Clusters\")\n",
    "        inertia = []\n",
    "        K = range(2, 21)\n",
    "        for k in K:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "            kmeans.fit(embeddings)\n",
    "            inertia.append(kmeans.inertia_)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        ax.plot(K, inertia, 'bx-')\n",
    "        ax.set_xlabel('Nombre de Clusters (k)')\n",
    "        ax.set_ylabel('Inertie')\n",
    "        ax.set_title('Méthode du Coude Pour Déterminer le Nombre Optimal de Clusters')\n",
    "        st.pyplot(fig)\n",
    "        plt.savefig(os.path.join(save_directory, \"elbow_method.png\"))\n",
    "        st.success(f\"Graphique de la méthode du coude enregistré : {os.path.join(save_directory, 'elbow_method.png')}\")\n",
    "\n",
    "        # Slider pour permettre à l'utilisateur de choisir le nombre de clusters\n",
    "        n_clusters = st.sidebar.slider(\"Choisissez le nombre de clusters\", 2, 20, 5, 1)\n",
    "\n",
    "        # Bouton pour lancer l'analyse KMeans\n",
    "        if st.button(\"Lancer l'Analyse KMeans\"):\n",
    "            cluster_model = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "            kmeans_labels = cluster_model.fit_predict(embeddings)\n",
    "\n",
    "            unique_clusters = len(set(kmeans_labels))\n",
    "            st.write(f\"Clusters KMeans trouvés : {unique_clusters} (regroupés)\")\n",
    "\n",
    "            if unique_clusters > 0:\n",
    "                # Visualisation des centroids des clusters\n",
    "                st.subheader(\"Visualisation des Centroides des Clusters K-Means\")\n",
    "                display_centroid_visualization(embeddings, kmeans_labels, save_directory)\n",
    "\n",
    "                # Visualisation des clusters en forme de bulle regroupés\n",
    "                st.subheader(\"Visualisation des Clusters Regroupés en Forme de Bulles\")\n",
    "                display_grouped_bubble_chart(embeddings, kmeans_labels, save_directory)\n",
    "\n",
    "                # Visualisation des clusters en 2D\n",
    "                st.subheader(\"Visualisation des Clusters en 2D\")\n",
    "                display_cluster_visualization(embeddings, kmeans_labels, save_directory)\n",
    "\n",
    "                st.subheader(\"Carte Thermique de Similarité des Clusters\")\n",
    "                display_similarity_matrix(embeddings, kmeans_labels, save_directory)\n",
    "\n",
    "                concordance_kmeans = create_concordance(df, kmeans_labels)\n",
    "                st.subheader(\"Concordancier KMeans\")\n",
    "                st.dataframe(concordance_kmeans)\n",
    "\n",
    "                save_csv(concordance_kmeans, \"concordance_kmeans\", save_directory)\n",
    "\n",
    "                display_wordclouds(df, kmeans_labels, save_directory)\n",
    "\n",
    "    elif menu_principal == \"FAQ\":\n",
    "        st.sidebar.markdown(\"### FAQ\")\n",
    "        st.subheader(\"FAQ : Analyse Textuelle avec K-Means++\")\n",
    "\n",
    "        st.markdown(\"\"\"\n",
    "        ### 1. Préparation des Données\n",
    "\n",
    "        Avant d'appliquer l'algorithme K-Means++, il est crucial de préparer correctement vos données. Le script a été conçu pour fonctionner avec des articles provenant de la plateforme **Europresse**, et est compatible avec le logiciel **Iramuteq**. Pour garantir un traitement adéquat, chaque article doit être précédé d'une ligne de démarcation commençant par `****`. Cette structure est essentielle pour que le script puisse identifier et traiter chaque article distinctement.\n",
    "\n",
    "        - **Format d'entrée :**\n",
    "          - **Fichiers Texte :** Les fichiers doivent être en format texte, avec des articles séparés par `****`.\n",
    "          - **URL :** Vous pouvez également fournir une URL d'où le texte sera extrait.\n",
    "\n",
    "        ### 2. Algorithme K-Means++\n",
    "\n",
    "        **K-Means++** est une amélioration de l'algorithme de clustering K-Means standard. Il est utilisé pour partitionner les données en un nombre fixe de groupes, appelés clusters. Voici une explication détaillée de son fonctionnement et de sa mise en œuvre dans votre script.\n",
    "\n",
    "        #### Comment Fonctionne K-Means++ :\n",
    "\n",
    "        - **Initialisation Améliorée :**\n",
    "          - Contrairement à l'initialisation aléatoire de K-Means, K-Means++ choisit les centroïdes initiaux de manière stratégique. Le premier centroïde est sélectionné aléatoirement, et les suivants sont choisis en fonction de leur distance par rapport aux centroïdes déjà sélectionnés, favorisant une répartition plus uniforme.\n",
    "\n",
    "        - **Assignation des Points :**\n",
    "          - Chaque point de données est assigné au cluster avec le centroïde le plus proche, généralement calculé avec la distance euclidienne.\n",
    "\n",
    "        - **Mise à jour des Centroïdes :**\n",
    "          - Pour chaque cluster, le centroïde est recalculé comme la moyenne de tous les points assignés à ce cluster.\n",
    "\n",
    "        - **Convergence :**\n",
    "          - L'algorithme répète les étapes d'assignation et de mise à jour jusqu'à ce que les centroïdes se stabilisent ou qu'un nombre maximal d'itérations soit atteint.\n",
    "\n",
    "        #### Visualisation des Résultats :\n",
    "\n",
    "        - **Centroides et Clusters :**\n",
    "          - Le script génère plusieurs graphiques pour visualiser les clusters et leurs centroïdes :\n",
    "            - **Graphique des Centroides :** Visualisation des positions moyennes des clusters après convergence.\n",
    "            - **Carte Thermique de Similarité :** Visualisation des similarités entre les clusters, basée sur la distance entre les centroïdes.\n",
    "            - **Nuages de Mots :** Mots-clés caractéristiques de chaque cluster, permettant de comprendre le contenu textuel de chaque groupe.\n",
    "\n",
    "        ### 3. Bibliothèques Python Utilisées :\n",
    "\n",
    "        - **Scikit-learn :** Pour l'implémentation de K-Means++, le calcul des clusters, et la gestion des données de texte.\n",
    "        - **SentenceTransformers :** Pour générer des embeddings à partir des textes, facilitant leur utilisation dans le clustering.\n",
    "        - **Matplotlib, Seaborn, Plotly :** Pour la création de graphiques et la visualisation des résultats.\n",
    "        - **WordCloud :** Pour la génération de nuages de mots permettant d'interpréter facilement les thèmes des clusters.\n",
    "\n",
    "        ### 4. Paramètres de K-Means++ :\n",
    "\n",
    "        - **`n_clusters` (Nombre de Clusters) :**\n",
    "          - **Description :** Indique le nombre de clusters que l'algorithme doit former. L'utilisateur doit déterminer cette valeur à l'avance.\n",
    "          - **Impact :** Un nombre trop élevé peut fragmenter des clusters naturels, tandis qu'un nombre trop bas peut regrouper des données disparates.\n",
    "          - **Détermination :** La méthode du coude est souvent utilisée pour déterminer le nombre optimal de clusters en traçant l'inertie (la somme des distances au carré entre les points et leur centroïde respectif) en fonction du nombre de clusters. Le point où l'inertie commence à diminuer moins rapidement (le coude) est considéré comme optimal.\n",
    "\n",
    "        - **`init` :**\n",
    "          - **Description :** Méthode d'initialisation des centroïdes.\n",
    "          - **Options :** Par défaut, `k-means++` est utilisé pour une meilleure convergence.\n",
    "          - **Impact :** L'initialisation K-Means++ permet d'éviter les mauvaises initialisations qui peuvent conduire à des solutions sous-optimales.\n",
    "\n",
    "        - **`max_iter` :**\n",
    "          - **Description :** Nombre maximal d'itérations pour une exécution de l'algorithme.\n",
    "          - **Impact :** Influence la durée de l'exécution. Un nombre d'itérations trop élevé peut entraîner une perte de temps, mais il est généralement suffisant pour atteindre la convergence.\n",
    "\n",
    "        - **`tol` :**\n",
    "          - **Description :** Tolérance pour la convergence. Détermine à quel point le déplacement des centroïdes doit être petit pour que l'algorithme s'arrête.\n",
    "\n",
    "        - **`random_state` :**\n",
    "          - **Description :** Assure la reproductibilité des résultats en fixant une graine pour la génération aléatoire.\n",
    "\n",
    "        #### Point Négatif : Détermination du Nombre de Clusters\n",
    "\n",
    "        - **Limitation :** Contrairement à des approches comme LDA (Latent Dirichlet Allocation) ou BERTopic, qui peuvent découvrir automatiquement des thèmes ou des topics, K-Means++ nécessite que l'utilisateur spécifie *a priori* le nombre de clusters. Cela peut être un inconvénient si l'utilisateur n'a pas une bonne intuition de la structure des données.\n",
    "\n",
    "        - **Comparaison avec LDA et BERTopic :** \n",
    "          - **LDA :** Identifie automatiquement les sujets dans un corpus textuel, utile pour des explorations sans connaissance préalable.\n",
    "          - **BERTopic :** Utilise des méthodes avancées pour découvrir des topics de manière plus flexible et peut s'adapter aux données sans pré-spécification du nombre de topics.\n",
    "\n",
    "        ### 5. Exemples d'Utilisation en Sciences Humaines\n",
    "\n",
    "        - **Analyse Textuelle :**\n",
    "          - **Regroupement de Documents :** Utiliser K-Means++ pour organiser des articles de presse en catégories thématiques, permettant ainsi une meilleure compréhension des sujets traités dans un grand volume de texte.\n",
    "          - **Segmentation des Utilisateurs :** Analyser les comportements ou préférences des utilisateurs en ligne, par exemple, pour regrouper des commentaires ou des avis en clusters distincts.\n",
    "\n",
    "        - **Sciences Humaines :**\n",
    "          - **Études Littéraires :** Clustering de corpus littéraires pour identifier des styles d'écriture ou des thèmes communs dans les œuvres de différents auteurs ou périodes.\n",
    "          - **Analyse Sociologique :** Identifier des groupes d'individus aux comportements ou opinions similaires à partir de données textuelles collectées via des enquêtes ou des réseaux sociaux.\n",
    "\n",
    "        #### Pourquoi Utiliser K-Means++ pour l'Analyse Textuelle ?\n",
    "\n",
    "        - **Simplicité et Efficacité :**\n",
    "          - Facile à comprendre et à implémenter. Idéal pour des tâches nécessitant une première segmentation des données.\n",
    "\n",
    "        - **Adaptabilité :**\n",
    "          - Fonctionne bien sur de grands ensembles de données textuelles, offrant une vue d'ensemble rapide et intuitive des clusters thématiques.\n",
    "\n",
    "        - **Complémentarité :**\n",
    "          - Peut être utilisé en complément d'autres méthodes d'analyse textuelle, comme l'analyse de la fréquence des termes ou les modèles de thématiques, pour affiner la compréhension des données.\n",
    "\n",
    "        ### Conclusion\n",
    "\n",
    "        K-Means++ est un algorithme puissant pour la segmentation des données, particulièrement utile dans l'analyse textuelle des sciences humaines. Bien que nécessitant une certaine intuition pour définir le nombre de clusters, ses résultats peuvent révéler des structures cachées dans les données et fournir une base solide pour des analyses plus approfondies.\n",
    "        \"\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
